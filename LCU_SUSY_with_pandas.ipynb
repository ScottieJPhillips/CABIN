{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79a008-ac8e-494e-bc85-b4219e60a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import h5py\n",
    "from os.path import exists\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "# Importing LearningCutsUtils\n",
    "from LearningCutsUtils import OneToOneLinear, EfficiencyScanNetwork\n",
    "from LearningCutsUtils import loss_fn, effic_loss_fn, lossvars\n",
    "import LearningCutsUtils.Utils as LCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2cc64-d24d-46fb-b46f-d77a8cb8817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Data using Pandas\n",
    "x_sig_data=None\n",
    "y_sig_data=None\n",
    "\n",
    "x_bkg_data=None\n",
    "y_bkg_data=None\n",
    "\n",
    "num_sig_events=0\n",
    "num_bkg_events=0\n",
    "\n",
    "# Need to copy the last branch, Rll, so that we have a total of 5 branches\n",
    "branches=(\n",
    "    'lep1MT_Met',\n",
    "    'lep2MT_Met',\n",
    "    'met_Et',\n",
    "    'Rll')\n",
    "\n",
    "# open signal\n",
    "mass=200\n",
    "split=30\n",
    "filepath='/data/mhance/SUSY/Compressed/'\n",
    "filebase='SusySkimSlep_v0.2_SlepSignals__'\n",
    "filename='MGPy8EG_A14N23LO_SlepSlep_dir_2L2MET75_%dp0_%dp0_NoSys' % (mass,mass-split)\n",
    "filesuff='.hf5'\n",
    "fullname=filepath+filebase+filename+filesuff\n",
    "\n",
    "new_branches = ('lep1MT_Met',\n",
    "    'lep2MT_Met',\n",
    "    'met_Et',\n",
    "    'Rll',\n",
    "    'Rll_lt')\n",
    "# Makes data frame for signal events\n",
    "num_sig_events = len(np.array(h5py.File(fullname)['MGPy8EG_A14N23LO_SlepSlep_dir_2L2MET75_200p0_170p0_NoSys'])[\"nJet30\"])\n",
    "df1 = pd.DataFrame(np.array(h5py.File(fullname)['MGPy8EG_A14N23LO_SlepSlep_dir_2L2MET75_200p0_170p0_NoSys']['lep1MT_Met', 'lep2MT_Met', 'met_Et','Rll']), columns=branches)\n",
    "df1[\"Rll_lt\"] = df1.iloc[:, -1]\n",
    "# Makes ndarray for xsignal using data frame\n",
    "x_sig_data = df1[:].values\n",
    "y_sig_data=np.ones(num_sig_events)\n",
    "\n",
    "print(fullname)\n",
    "print(\"Extracted %7d signal events\" % num_sig_events)\n",
    "\n",
    "fullname=filepath+\"SusySkimSlep_v0.2_diboson2L__diboson2L_NoSys\"+filesuff\n",
    "df2 = pd.DataFrame(np.array(h5py.File(fullname)['diboson2L_NoSys']['lep1MT_Met', 'lep2MT_Met', 'met_Et','Rll']), columns=branches)\n",
    "df2[\"Rll_lt\"] = df2.iloc[:, -1]\n",
    "\n",
    "num_events=0\n",
    "\n",
    "num_bkg_events=len(np.array(h5py.File(fullname)['diboson2L_NoSys'][\"nJet30\"]))\n",
    "x_bkg_data = df2[:].values\n",
    "y_bkg_data=np.zeros(num_bkg_events)\n",
    "print(\"Extracted %7d background events\" % num_bkg_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db4114-92ca-4efe-abf1-6ff6885254d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=None\n",
    "y_data=None\n",
    "if num_bkg_events>num_sig_events:\n",
    "    x_data = np.concatenate((x_sig_data,x_bkg_data[:num_sig_events]))\n",
    "    y_data = np.concatenate((y_sig_data,y_bkg_data[:num_sig_events]))\n",
    "else:\n",
    "    x_data = np.concatenate((x_sig_data[:num_bkg_events],x_bkg_data))\n",
    "    y_data = np.concatenate((y_sig_data[:num_bkg_events],y_bkg_data))\n",
    "\n",
    "print(y_sig_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fd8f8-522e-4c96-b51a-02af0e8e531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read in the data as fields with a custom format, which is useful for keeping track of what's what, but \n",
    "# ML libraries wants everything as tuples of floats.  \n",
    "#x_data=[tuple(float(i) if np.isfinite(float(i)) else 0 for i in j) for j in x_data]\n",
    "x_data=[tuple(float(i) for i in j) for j in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0403e-2441-418b-8d47-c5ea3f273ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, \n",
    "                                                    test_size=int(0.1*len(x_data)), \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c313825-4e99-49bf-8042-1e8ef6d0a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_before_scaling={}\n",
    "for b in new_branches:\n",
    "    x_train_before_scaling[b]=[event[new_branches.index(b)] for event in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca0a91-0848-4cc6-951c-3ada7509bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,30))\n",
    "fig.tight_layout()\n",
    "for b in range(len(new_branches)):\n",
    "    ax=fig.add_subplot(10,5,1+b)\n",
    "    plt.subplots_adjust(hspace=0.3,wspace=0.5)\n",
    "    ax.hist(x_train_before_scaling[new_branches[b]])\n",
    "    ax.set_xlabel(new_branches[b])\n",
    "    ax.set_ylabel(\"Events/Bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fac02-0041-44c1-a41b-b29cf91020e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20000 # number of points\n",
    "m=5 # dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1ccf0-84cc-4247-a37b-8f51b55974e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now scale based on the training data:\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7bb1c-c92d-4cfe-8b75-df8fa2e110b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_after_scaling={}\n",
    "for b in new_branches:\n",
    "    x_train_after_scaling[b]=[event[new_branches.index(b)] for event in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8c5d7-a740-44b0-8392-7976427acdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,30))\n",
    "fig.tight_layout()\n",
    "for b in range(len(new_branches)):\n",
    "    ax=fig.add_subplot(10,5,1+b)\n",
    "    plt.subplots_adjust(hspace=0.3,wspace=0.5)\n",
    "    ax.hist(x_train_after_scaling[new_branches[b]])\n",
    "    ax.set_xlabel(new_branches[b])\n",
    "    ax.set_ylabel(\"Events/Bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086a9ce-4334-4f8a-a2bc-c31633663aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_sig_data = np.transpose(x_sig_data)\n",
    "\n",
    "x_bkg_data = np.transpose(x_bkg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04337c-85e6-48f1-a859-c7b3d073ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = plt.figure(figsize=(20,5))\n",
    "fig.tight_layout()\n",
    "nbins=50\n",
    "\n",
    "for b in range(len(new_branches)):\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax=fig.add_subplot(2,5,1+b)\n",
    "    plt.subplots_adjust(hspace=0.3,wspace=0.5)\n",
    "    plt.yscale('log')\n",
    "    ax.hist(x_sig_data[b],nbins,density=True,histtype='stepfilled',alpha=0.5,color='red')\n",
    "    ax.hist(x_bkg_data[b],nbins,density=True,histtype='stepfilled',alpha=0.5,color='blue')\n",
    "    ax.set_xlabel(f\"Feature {new_branches[b]}\")\n",
    "    ax.set_ylabel(\"Events/Bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4856de-9a71-4696-8e39-da190df23649",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(len(branches), 20),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(20, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 1)\n",
    ")\n",
    "torch.save(net.state_dict(), 'net.pth')\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.05, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27e1aa-156b-489d-ae44-a9cba7c0712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor=torch.tensor(x_train,dtype=torch.float)\n",
    "y_train_tensor=torch.tensor(y_train,dtype=torch.float)\n",
    "y_train_tensor=y_train_tensor.unsqueeze(1)\n",
    "\n",
    "print(x_train_tensor.shape)\n",
    "\n",
    "x_test_tensor=torch.tensor(x_test,dtype=torch.float)\n",
    "y_test_tensor=torch.tensor(y_test,dtype=torch.float)\n",
    "y_test_tensor=y_test_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9e9d2-6c7b-4ec7-9aad-52855ae76a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import LearningCutsUtils.LearningCutsUtils\n",
    "## needed if we change LearningCutsUtils and want to avoid reloading the kernel to see the effects\n",
    "importlib.reload(LearningCutsUtils.LearningCutsUtils)\n",
    "import LearningCutsUtils.LearningCutsUtils as LCU\n",
    "from LearningCutsUtils import loss_fn\n",
    "from LearningCutsUtils import effic_loss_fn\n",
    "\n",
    "\n",
    "N=20000 # number of points\n",
    "m=5 # dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8de5d-1cc8-4552-978d-d674d51d272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt=-1.\n",
    "lt=1.\n",
    "\n",
    "cuts_gt_lt = [lt, lt, gt, gt, lt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17e90d-2de8-4816-a1d1-9d217d2af0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_loop(activation_input_scale_factor=15., learning_rate=0.1, batch_size=int(len(y_train)/20.), epochs = 50, alpha = 10., beta=0.1, gamma=1e-2, target_efficiency=0.8):\n",
    "    net = OneToOneLinear(m,activation_input_scale_factor,cuts_gt_lt)\n",
    "    torch.save(net.state_dict(), 'net_learningbiases.pth')\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    {n: theta.shape for n, theta in net.named_parameters()}\n",
    "    losses = []\n",
    "    losses_test = []\n",
    "\n",
    "    net.load_state_dict(torch.load('net_learningbiases.pth',weights_only=True))\n",
    "\n",
    "    xy_train = torch.utils.data.TensorDataset(x_train_tensor.float(),y_train_tensor)\n",
    "    loader = torch.utils.data.DataLoader(xy_train, batch_size=batch_size, shuffle=True)\n",
    "    debug=False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        start_time = time.time()\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = net(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred, y_batch.squeeze(1), m, net, target_efficiency, alpha, beta, gamma)\n",
    "            loss.totalloss().backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss)\n",
    "        net.eval() # configure the model for evaluation (testing)\n",
    "        y_pred = net(x_test_tensor)\n",
    "        test_loss =loss_fn(y_pred, y_test_tensor.squeeze(1), m, net, target_efficiency, alpha, beta, gamma)\n",
    "        losses_test.append(test_loss)\n",
    "        end_time=time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        bias=net.bias[0]\n",
    "        weight=net.weight[0]\n",
    "        #weight={weight:4.1e}, bias={bias:4.1e}, \n",
    "        print(f\"Completed epoch {epoch:2d} in {elapsed_time:4.1f}s, Train loss={loss.totalloss().data:4.1e}, Test loss={test_loss.totalloss().data:4.1e}, cut={-bias/weight:4.1e}, sig_eff={100*test_loss.signaleffic:4.1f}%, bkg_eff={100*test_loss.backgreffic:6.3f}%\")\n",
    "    net.eval() # configure the model for evaluation (testing)\n",
    "    y_pred_test = net(x_test_tensor).detach().cpu()\n",
    "    y_pred_train= net(x_train_tensor).detach().cpu()\n",
    "    LearningCutsUtils.LearningCutsUtils.make_ROC_curve(y_test, y_pred_test)\n",
    "    return y_pred_test, y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7775e-f0f0-45ac-993a-374570b0fe04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test, y_pred_train = function_to_loop(activation_input_scale_factor=15., learning_rate=0.5, batch_size=int(len(y_train)/1.), epochs = 150, alpha = 10., beta=0.1, gamma=10**(-5), target_efficiency=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea74e2-ae71-4d4e-b5e0-d265fd75fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCU.plot_classifier_output(y_train, y_pred_train, y_test, y_pred_test, nbins=20, range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a1be5-8adf-4e83-82cb-d5933f0a2acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
